import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Embedding, LSTM
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import json

# Step 1: Load the JSONL dataset
# This function reads a JSONL file and converts each line into a Python dictionary,
# appending them to a list and finally returning a DataFrame.
def load_jsonl(file_path):
    data = []
    with open(file_path, 'r') as f:
        for line in f:
            data.append(json.loads(line))
    return pd.DataFrame(data)

# Load train, validation, and test datasets
# Adjust file paths based on your local environment
train_df = load_jsonl('D:/python/codes/train.jsonl')
valid_df = load_jsonl('D:/python/codes/valid.jsonl')
test_df = load_jsonl('D:/python/codes/test.jsonl')

# Step 2: Preprocessing the dataset
# Combine train and validation datasets to form a larger training set
df = pd.concat([train_df, valid_df])

# Inspect column names to ensure you know the correct names of the fields
# This step helps identify which columns hold the code and labels
print(df.columns)

# Extract code snippets and labels for further processing
# Replace 'code' and 'status' with the actual column names from your dataset
texts = df['code'].values  # Assuming 'code' column holds the code snippets
labels = df['status'].values  # Assuming 'status' column is the binary label (vulnerable/not)

# Encode labels (e.g., Yes/No -> 1/0) for use in the model
label_encoder = LabelEncoder()
labels = label_encoder.fit_transform(labels)

# Step 3: Tokenize the source code
# Convert the raw code snippets into tokenized sequences (numerical representations of words)
tokenizer = Tokenizer(num_words=5000, oov_token="<OOV>")
tokenizer.fit_on_texts(texts)  # Fit the tokenizer on the entire dataset
sequences = tokenizer.texts_to_sequences(texts)  # Convert texts to sequences of integers

# Pad the sequences to ensure uniform length (maxlen), truncating or padding as necessary
maxlen = 100
padded_sequences = pad_sequences(sequences, padding='post', maxlen=maxlen)

# Split the data into training and testing sets (80/20 split)
X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)

# Step 4: Build the deep learning model
# We use an embedding layer to convert tokenized words into dense vectors,
# followed by an LSTM layer for handling sequential data, and Dense layers for binary classification
embedding_dim = 100  # Size of the embedding vectors
model = Sequential()
model.add(Embedding(input_dim=5000, output_dim=embedding_dim, input_length=maxlen))  # Embedding layer
model.add(LSTM(128, return_sequences=False))  # LSTM layer with 128 units
model.add(Dense(64, activation='relu'))  # Fully connected layer with ReLU activation
model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification (vulnerable or not)

# Compile the model with binary crossentropy loss and Adam optimizer
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Step 5: Train the model
# Train the model on the training data and validate on the test data
# Epochs control how many times the model sees the entire dataset
model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), batch_size=32)

# Step 6: Evaluate the model
# After training, evaluate the model's performance on the test data
loss, accuracy = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {accuracy * 100:.2f}%")  # Print test accuracy in percentage

# Step 7: Predict vulnerabilities on new code samples
# Use the trained model to predict whether new code samples are vulnerable
new_code_sample = "int main() { int a = 0; while(a !=  10) { a++; } return 0; }"
new_sequence = tokenizer.texts_to_sequences([new_code_sample])  # Tokenize the new code sample
new_padded_sequence = pad_sequences(new_sequence, maxlen=maxlen)  # Pad it to the same max length

# Predict whether the new sample is vulnerable
prediction = model.predict(new_padded_sequence)
vulnerable = prediction[0][0] > 0.5  # Threshold: if prediction > 0.5, it's classified as vulnerable
print(f"Vulnerability Prediction: {'Yes' if vulnerable else 'No'}")
